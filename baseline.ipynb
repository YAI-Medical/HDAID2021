{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Heart Disease Artificial Intelligence Datathon 2021\n",
    "\n",
    "**Baseline Code**\n",
    "\n",
    "## 필독 TODO\n",
    "\n",
    "* 해당 baseline 모델 및 오차 함수들은 Multiclass 및 One-Hot 라벨을 가정하고 만들어졌습니다.\n",
    "\n",
    "* 따라서 그대로 사용하시기보단 loss function 튜닝이 필요합니다. (PR로 만들어주세요.)\n",
    "\n",
    "* 그리고 dataset root path 설정해야 합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Runtime Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## View Runtime Information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0 or gpu_info.find('not found') >= 0:\n",
    "    device = 'cpu'; print('Not connected to a GPU')\n",
    "else: device = 'cuda'; print(gpu_info)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print(f'Your runtime has {ram_gb:.1f} gigabytes of available RAM\\n'\n",
    "      f'{\"Not\" if ram_gb < 20 else \"You are\"} using a high-RAM runtime!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "print(f\"OS version: \\t\\t{platform.platform()}\\n\"\n",
    "      f\"Python version:\\t\\t{sys.version.replace(chr(10), str())}\\n\"\n",
    "      f\"Torch version:\\t\\t{torch.__version__}\\n\"\n",
    "      f\"Torch device:\\t\\t{device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare device and library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# After all installation, import all libraries used.\n",
    "\n",
    "!pip install torchinfo\n",
    "!pip install pyclean\n",
    "!pyclean .\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset, RandomSampler, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchinfo\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make Dataset Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.dataset import ImageList"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "root: str = ...\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_a2c = os.path.join(root, 'train', 'A2C')\n",
    "train_a2c = ImageList.from_path(train_a2c, transform=transform, target_transform=transform)\n",
    "\n",
    "train_a4c = os.path.join(root, 'train', 'A4C')\n",
    "train_a4c = ImageList.from_path(train_a4c, transform=transform, target_transform=transform)\n",
    "\n",
    "val_a2c = os.path.join(root, 'validation', 'A2C')\n",
    "val_a2c = ImageList.from_path(val_a2c, transform=transform, target_transform=transform)\n",
    "\n",
    "val_a4c = os.path.join(root, 'validation', 'A4C')\n",
    "val_a4c = ImageList.from_path(val_a4c, transform=transform, target_transform=transform)\n",
    "\n",
    "train_datasets = ConcatDataset([train_a2c, train_a4c])\n",
    "val_datasets = ConcatDataset([val_a2c, val_a4c])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation Network\n",
    "\n",
    "* **DeepLabV3 + Resnet101**: Baseline Model\n",
    "\n",
    "* **U-Net**\n",
    "\n",
    "* **Inception U-Net**\n",
    "\n",
    "* **RefineNet**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "\n",
    "from models.unet import UNet, InceptionUNet\n",
    "from models.refinenet import refinenet50, refinenet101, refinenet152, rf_lw50, rf_lw101, rf_lw152"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Baseline: DeeplabV3 + ResNet101\n",
    "\n",
    "# # Pretrained Model\n",
    "net = deeplabv3_resnet101(pretrained=True, progress=False)\n",
    "net.classifier = DeepLabHead(2048, 2)\n",
    "# net.aux_classifier = nn.Sequential()\n",
    "net.aux_classifier = FCNHead(1024, 2)\n",
    "\n",
    "# # Non-pretrained Model\n",
    "# net = deeplabv3_resnet101(pretrained=False, num_classes=6)\n",
    "\n",
    "trainable_backbone_layers = ['layer4']\n",
    "for n, p in net.named_parameters():\n",
    "    if n.startswith('backbone') and n.split('.')[1] not in trainable_backbone_layers:\n",
    "        p.requires_grad = False\n",
    "\n",
    "net.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "torchinfo.summary(net, (1, 3, 256, 256))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Network\n",
    "\n",
    "* **Binary Cross Entropy**\n",
    "\n",
    "* **Dice Coefficient**\n",
    "\n",
    "* **Intersection over Union Score**\n",
    "\n",
    "- More Multi-Label Segmentation Losses: https://jeune-research.tistory.com/entry/Loss-Functions-for-Image-Segmentation-Region-Based-Losses\n",
    "\n",
    "- See also: https://smp.readthedocs.io/en/latest/losses.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.loss import BCEDiceIoUWithLogitsLoss2d, BCEDiceIoULoss2d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Hyper Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.lr_scheduler import CosineAnnealingWarmUpRestarts\n",
    "\n",
    "# Lazy-eval iterable dataset: do not set sampler or shuffle options\n",
    "num_epoch = 100\n",
    "\n",
    "batch_size = 35\n",
    "num_workers = 1\n",
    "\n",
    "loss_function = BCEDiceIoUWithLogitsLoss2d()\n",
    "optimizer_class = torch.optim.Adam\n",
    "optimizer_config = {'lr': 1e-6}\n",
    "scheduler_class = CosineAnnealingWarmUpRestarts\n",
    "scheduler_config = {'T_0': 10, 'T_mult': 2, 'eta_max': 1e-3, 'T_up': 3, 'gamma': 0.5}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and Evaluate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_datasets, batch_size, num_workers=num_workers, drop_last=False)\n",
    "val_loader = DataLoader(val_datasets, batch_size, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "optimizer = optimizer_class(net.parameters(), **optimizer_config)\n",
    "lr_scheduler = scheduler_class(optimizer, **scheduler_config)\n",
    "\n",
    "\n",
    "def load_state_dict(d):\n",
    "    net.load_state_dict(d['model'])\n",
    "    optimizer.load_state_dict(d['optimizer'])\n",
    "    lr_scheduler.load_state_dict(d['lr_scheduler'])\n",
    "\n",
    "\n",
    "def state_dict():\n",
    "    from collections import OrderedDict\n",
    "    d = OrderedDict()\n",
    "    d['model'] = net.state_dict()\n",
    "    d['optimizer'] = optimizer.state_dict()\n",
    "    d['lr_scheduler'] = lr_scheduler.state_dict()\n",
    "    return d\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import uuid\n",
    "from utils.training import train_one_epoch\n",
    "\n",
    "try:\n",
    "    print(f\"Re-using session: {session_name}\")\n",
    "except NameError:\n",
    "    session_name = str(uuid.uuid4())\n",
    "    print(f\"Generating session: {session_name}\")\n",
    "\n",
    "checkpoint_dir = f'checkpoint/{session_name}'\n",
    "os.makedirs('checkpoint', exist_ok=True)\n",
    "\n",
    "for ep in range(num_epoch):\n",
    "    train_one_epoch(net, loss_function, optimizer, lr_scheduler, train_loader, val_loader, device, ep, warmup_start=False)\n",
    "    torch.save(state_dict(), os.path.join(checkpoint_dir, '{}.pt').format(ep))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TBD\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}